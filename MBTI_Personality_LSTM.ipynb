{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "A93XdP1pqy2Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.layers import (\n",
        "    Input, Embedding, Bidirectional, LSTM,\n",
        "    Dense, Dropout, Softmax, Lambda, Layer\n",
        ")\n",
        "from sklearn.metrics import f1_score, roc_auc_score\n",
        "from sklearn.preprocessing import label_binarize\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ncGjU95aM0M",
        "outputId": "5ab41c4b-5479-4a48-d5da-f87056caf6d3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NLTK Setup"
      ],
      "metadata": {
        "id": "Q-41Wy1mq7h2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'https?://\\S+|www\\.\\S+', ' ', text)\n",
        "    text = re.sub(r'[^a-z0-9\\s]', ' ', text)\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [lemmatizer.lemmatize(tok) for tok in tokens if tok not in stop_words]\n",
        "    return ' '.join(tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WwGnTyd5aSRl",
        "outputId": "20503d83-421b-4c37-c16a-c95cb62f4d19"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load & Preprocess Data\n",
        "\n"
      ],
      "metadata": {
        "id": "0ZUmBILNrBN8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('/content/drive/MyDrive/ML_Project/mbti_1.csv')\n",
        "data['clean_posts'] = data['posts'].apply(preprocess_text)\n",
        "\n",
        "train_df, test_df = train_test_split(data, test_size=0.25, stratify=data['type'], random_state=42)\n"
      ],
      "metadata": {
        "id": "KDRoLTjjaZqk"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenize & Pad"
      ],
      "metadata": {
        "id": "JrAaQljXrGAp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(num_words=10000)\n",
        "tokenizer.fit_on_texts(train_df['clean_posts'])\n",
        "\n",
        "X_train = tokenizer.texts_to_sequences(train_df['clean_posts'])\n",
        "X_test = tokenizer.texts_to_sequences(test_df['clean_posts'])\n",
        "\n",
        "max_len = max(len(x) for x in X_train)\n",
        "X_train = pad_sequences(X_train, maxlen=max_len, padding='post')\n",
        "X_test = pad_sequences(X_test, maxlen=max_len, padding='post')"
      ],
      "metadata": {
        "id": "EyniU5LTacIi"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encode Targets"
      ],
      "metadata": {
        "id": "ELW6fxXqrQNI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "le = LabelEncoder()\n",
        "y_train = le.fit_transform(train_df['type'])\n",
        "y_test = le.transform(test_df['type'])\n",
        "\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
        "class_weights = dict(enumerate(class_weights))"
      ],
      "metadata": {
        "id": "8cFEeZ0uaeMe"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load GloVe Embeddings"
      ],
      "metadata": {
        "id": "XriTea4WrVUw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_glove(path, dim=100):\n",
        "    embeddings_index = {}\n",
        "    with open(path, encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            vector = np.asarray(values[1:], dtype='float32')\n",
        "            embeddings_index[word] = vector\n",
        "    return embeddings_index\n",
        "\n",
        "glove_path = '/content/drive/MyDrive/ML_Project/glove.6B.100d.txt'  # Update path\n",
        "embedding_dim = 100\n",
        "glove_embeddings = load_glove(glove_path, dim=embedding_dim)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    if word in glove_embeddings:\n",
        "        embedding_matrix[i] = glove_embeddings[word]\n"
      ],
      "metadata": {
        "id": "8hTFeJPMagZH"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Attention Layer"
      ],
      "metadata": {
        "id": "BKl4pu5LrZLG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionLayer(Layer):\n",
        "    def build(self, input_shape):\n",
        "        self.W = self.add_weight(shape=(input_shape[-1], 1), initializer='glorot_uniform', trainable=True)\n",
        "        super().build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        score = tf.nn.tanh(tf.tensordot(inputs, self.W, axes=1))\n",
        "        attention_weights = tf.nn.softmax(score, axis=1)\n",
        "        context = tf.reduce_sum(attention_weights * inputs, axis=1)\n",
        "        return context"
      ],
      "metadata": {
        "id": "CaM68cFZai2a"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build Custom LSTM Model"
      ],
      "metadata": {
        "id": "ZEUwSW6JrdP2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(vocab_size, embedding_dim, max_len, num_classes):\n",
        "    input_layer = Input(shape=(max_len,))\n",
        "    embedding_layer = Embedding(input_dim=vocab_size,\n",
        "                                output_dim=embedding_dim,\n",
        "                                weights=[embedding_matrix],\n",
        "                                input_length=max_len,\n",
        "                                trainable=False)(input_layer)\n",
        "\n",
        "    x = Bidirectional(LSTM(128, return_sequences=True))(embedding_layer)\n",
        "    x = Dropout(0.5)(x)\n",
        "    x = AttentionLayer()(x)\n",
        "    x = Dense(64, activation='relu')(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    output_layer = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    model = Model(inputs=input_layer, outputs=output_layer)\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4, clipnorm=1.0)\n",
        "    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "model = build_model(len(word_index)+1, embedding_dim, max_len, len(le.classes_))\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        },
        "id": "2I3CGg3ZanSr",
        "outputId": "bb2ae6a3-a9c0-41cb-a462-270ed9d93ac6"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m862\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m862\u001b[0m, \u001b[38;5;34m100\u001b[0m)       │     \u001b[38;5;34m8,409,100\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ bidirectional (\u001b[38;5;33mBidirectional\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m862\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │       \u001b[38;5;34m234,496\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m862\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ attention_layer                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │           \u001b[38;5;34m256\u001b[0m │\n",
              "│ (\u001b[38;5;33mAttentionLayer\u001b[0m)                │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m16,448\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │         \u001b[38;5;34m1,040\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">862</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">862</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">8,409,100</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">862</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">234,496</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">862</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ attention_layer                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AttentionLayer</span>)                │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,448</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,040</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m8,661,340\u001b[0m (33.04 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,661,340</span> (33.04 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m252,240\u001b[0m (985.31 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">252,240</span> (985.31 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m8,409,100\u001b[0m (32.08 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,409,100</span> (32.08 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True),\n",
        "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2)\n",
        "]\n",
        "\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_test, y_test),\n",
        "    epochs=20,\n",
        "    batch_size=64,\n",
        "    class_weight=class_weights,\n",
        "    callbacks=callbacks\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3KHA1DRUaqI0",
        "outputId": "8a9c388d-6d34-456a-9907-182324d8a4b4"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 93ms/step - accuracy: 0.1307 - loss: 2.6667 - val_accuracy: 0.1355 - val_loss: 2.7619 - learning_rate: 5.0000e-05\n",
            "Epoch 2/20\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 92ms/step - accuracy: 0.0811 - loss: 2.7603 - val_accuracy: 0.0489 - val_loss: 2.7622 - learning_rate: 5.0000e-05\n",
            "Epoch 3/20\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 95ms/step - accuracy: 0.0818 - loss: 2.8025 - val_accuracy: 0.1577 - val_loss: 2.7581 - learning_rate: 5.0000e-05\n",
            "Epoch 4/20\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 93ms/step - accuracy: 0.0977 - loss: 2.7684 - val_accuracy: 0.0493 - val_loss: 2.7575 - learning_rate: 5.0000e-05\n",
            "Epoch 5/20\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 95ms/step - accuracy: 0.0761 - loss: 2.7655 - val_accuracy: 0.1853 - val_loss: 2.7548 - learning_rate: 5.0000e-05\n",
            "Epoch 6/20\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 103ms/step - accuracy: 0.1060 - loss: 2.7846 - val_accuracy: 0.0788 - val_loss: 2.7533 - learning_rate: 5.0000e-05\n",
            "Epoch 7/20\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 96ms/step - accuracy: 0.1073 - loss: 2.7287 - val_accuracy: 0.1282 - val_loss: 2.7469 - learning_rate: 5.0000e-05\n",
            "Epoch 8/20\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 95ms/step - accuracy: 0.1317 - loss: 2.7323 - val_accuracy: 0.1300 - val_loss: 2.7286 - learning_rate: 5.0000e-05\n",
            "Epoch 9/20\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 97ms/step - accuracy: 0.1145 - loss: 2.7517 - val_accuracy: 0.1153 - val_loss: 2.6925 - learning_rate: 5.0000e-05\n",
            "Epoch 10/20\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 98ms/step - accuracy: 0.1242 - loss: 2.7557 - val_accuracy: 0.1591 - val_loss: 2.6847 - learning_rate: 5.0000e-05\n",
            "Epoch 11/20\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 98ms/step - accuracy: 0.1242 - loss: 2.7145 - val_accuracy: 0.1438 - val_loss: 2.6720 - learning_rate: 5.0000e-05\n",
            "Epoch 12/20\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 98ms/step - accuracy: 0.1290 - loss: 2.7092 - val_accuracy: 0.1766 - val_loss: 2.6548 - learning_rate: 5.0000e-05\n",
            "Epoch 13/20\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 96ms/step - accuracy: 0.1641 - loss: 2.7095 - val_accuracy: 0.1346 - val_loss: 2.6428 - learning_rate: 5.0000e-05\n",
            "Epoch 14/20\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 98ms/step - accuracy: 0.1464 - loss: 2.6660 - val_accuracy: 0.1581 - val_loss: 2.6087 - learning_rate: 5.0000e-05\n",
            "Epoch 15/20\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 97ms/step - accuracy: 0.1722 - loss: 2.6834 - val_accuracy: 0.2019 - val_loss: 2.5942 - learning_rate: 5.0000e-05\n",
            "Epoch 16/20\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 97ms/step - accuracy: 0.1642 - loss: 2.5980 - val_accuracy: 0.1664 - val_loss: 2.5899 - learning_rate: 5.0000e-05\n",
            "Epoch 17/20\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 97ms/step - accuracy: 0.1756 - loss: 2.5905 - val_accuracy: 0.1987 - val_loss: 2.5448 - learning_rate: 5.0000e-05\n",
            "Epoch 18/20\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 97ms/step - accuracy: 0.1714 - loss: 2.5691 - val_accuracy: 0.1577 - val_loss: 2.6602 - learning_rate: 5.0000e-05\n",
            "Epoch 19/20\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 98ms/step - accuracy: 0.1703 - loss: 2.7154 - val_accuracy: 0.1830 - val_loss: 2.5523 - learning_rate: 5.0000e-05\n",
            "Epoch 20/20\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 98ms/step - accuracy: 0.1619 - loss: 2.5773 - val_accuracy: 0.2310 - val_loss: 2.4961 - learning_rate: 2.5000e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation metrics"
      ],
      "metadata": {
        "id": "m5zBjZa9rjHK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hXyEnVm6cCmM",
        "outputId": "02da727b-b9cf-4bdd-dd91-33c0c0f354fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 29ms/step - accuracy: 0.2170 - loss: 2.5084\n",
            "\n",
            "Test Loss: 2.4960, Test Accuracy: 0.2310\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 29ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        ENFJ       0.09      0.29      0.14        48\n",
            "        ENFP       0.08      0.01      0.01       169\n",
            "        ENTJ       0.05      0.21      0.08        58\n",
            "        ENTP       0.16      0.17      0.16       171\n",
            "        ESFJ       0.00      0.00      0.00        10\n",
            "        ESFP       0.00      0.00      0.00        12\n",
            "        ESTJ       0.00      0.00      0.00        10\n",
            "        ESTP       0.00      0.00      0.00        22\n",
            "        INFJ       0.24      0.06      0.09       368\n",
            "        INFP       0.39      0.47      0.42       458\n",
            "        INTJ       0.31      0.06      0.10       273\n",
            "        INTP       0.25      0.52      0.34       326\n",
            "        ISFJ       0.00      0.00      0.00        41\n",
            "        ISFP       0.08      0.07      0.08        68\n",
            "        ISTJ       0.10      0.02      0.03        51\n",
            "        ISTP       0.19      0.20      0.20        84\n",
            "\n",
            "    accuracy                           0.23      2169\n",
            "   macro avg       0.12      0.13      0.10      2169\n",
            "weighted avg       0.23      0.23      0.20      2169\n",
            "\n",
            "Confusion Matrix:\n",
            " [[ 14   0   2   4   0   0   0   1   4  17   2   1   0   1   0   2]\n",
            " [ 29   1  12  10   1   0   0   0   7  57   1  30   0  15   1   5]\n",
            " [  4   0  12  10   0   0   0   0   3   6   2  19   0   1   0   1]\n",
            " [ 10   0  21  29   0   0   0   1   6   8   5  75   0   7   2   7]\n",
            " [  1   0   2   0   0   0   0   0   3   2   0   2   0   0   0   0]\n",
            " [  2   0   0   2   0   0   0   0   2   2   0   3   0   0   0   1]\n",
            " [  0   0   3   3   0   0   0   0   0   3   0   1   0   0   0   0]\n",
            " [  3   0   1   4   0   0   0   0   0   0   0   3   0   2   1   8]\n",
            " [ 33   3  31  20   0   0   0   4  21 138   6 100   0   5   1   6]\n",
            " [ 36   8  23   7   1   0   0   3  24 214   6 107   0  10   1  18]\n",
            " [  4   0  69  35   1   0   0   3   9  18  17 101   0   6   2   8]\n",
            " [  3   0  54  30   0   1   0   2   5  26  13 170   0   6   1  15]\n",
            " [  6   1   3   2   0   0   0   1   1  18   1   7   0   1   0   0]\n",
            " [  5   0   1   4   0   1   0   0   2  28   0  20   0   5   0   2]\n",
            " [  1   0  11  11   0   0   0   0   0   9   0  15   0   3   1   0]\n",
            " [  1   0  10  14   0   1   0   0   1   9   2  28   0   1   0  17]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "loss, acc = model.evaluate(X_test, y_test)\n",
        "print(f\"\\nTest Loss: {loss:.4f}, Test Accuracy: {acc:.4f}\")\n",
        "\n",
        "y_pred = np.argmax(model.predict(X_test), axis=1)\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "print(classification_report(y_test, y_pred, target_names=le.classes_))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "macro_f1 = f1_score(y_test, y_pred, average='macro')\n",
        "weighted_f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "print(f\"\\nMacro F1-score: {macro_f1:.4f}\")\n",
        "print(f\"Weighted F1-score: {weighted_f1:.4f}\")\n",
        "\n",
        "y_test_bin = label_binarize(y_test, classes=np.arange(len(le.classes_)))\n",
        "y_pred_proba = model.predict(X_test)\n",
        "\n",
        "try:\n",
        "    auc_macro = roc_auc_score(y_test_bin, y_pred_proba, average='macro', multi_class='ovr')\n",
        "    print(f\"AUC-ROC (macro): {auc_macro:.4f}\")\n",
        "except ValueError as e:\n",
        "    print(\"AUC-ROC could not be computed:\", e)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qN6etIYiePeF",
        "outputId": "4d71b789-cdb2-4121-8d9b-ed89bb95958c"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Macro F1-score: 0.1032\n",
            "Weighted F1-score: 0.1982\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step\n",
            "AUC-ROC (macro): 0.6998\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conf_mat = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=le.classes_,\n",
        "            yticklabels=le.classes_)\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xticks(rotation=45)\n",
        "plt.yticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Hvpw9jnIeQMw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load & Preprocess Data for Biany Classification"
      ],
      "metadata": {
        "id": "ktGEFnF9rtqD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('/content/drive/MyDrive/ML_Project/mbti_1.csv')\n",
        "data['clean_posts'] = data['posts'].apply(preprocess_text)\n",
        "\n",
        "train_df, test_df = train_test_split(data, test_size=0.25, stratify=data['type'], random_state=42)"
      ],
      "metadata": {
        "id": "h3Hu1uvSbyeV"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for df in (train_df, test_df):\n",
        "    df['IE'] = df['type'].str[0].map({'I':0, 'E':1})\n",
        "    df['SN'] = df['type'].str[1].map({'S':0, 'N':1})\n",
        "    df['TF'] = df['type'].str[2].map({'T':0, 'F':1})\n",
        "    df['JP'] = df['type'].str[3].map({'J':0, 'P':1})\n",
        "\n",
        "y_train = { t: train_df[t].values for t in ['IE','SN','TF','JP'] }\n",
        "y_test  = { t: test_df[t].values  for t in ['IE','SN','TF','JP'] }\n"
      ],
      "metadata": {
        "id": "bb1OLYcoawiy"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(num_words=10000)\n",
        "tokenizer.fit_on_texts(train_df['clean_posts'])\n",
        "\n",
        "X_train_seq = tokenizer.texts_to_sequences(train_df['clean_posts'])\n",
        "X_test_seq  = tokenizer.texts_to_sequences(test_df['clean_posts'])\n",
        "\n",
        "max_len = max(len(s) for s in X_train_seq)\n",
        "X_train = pad_sequences(X_train_seq, maxlen=max_len, padding='post')\n",
        "X_test  = pad_sequences(X_test_seq,  maxlen=max_len, padding='post')\n",
        "\n",
        "word_index = tokenizer.word_index"
      ],
      "metadata": {
        "id": "PbRf9HyHb26K"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_glove(path, dim=100):\n",
        "    idx = {}\n",
        "    with open(path, encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            parts = line.split()\n",
        "            word, vec = parts[0], np.asarray(parts[1:], 'float32')\n",
        "            idx[word] = vec\n",
        "    return idx\n",
        "\n",
        "glove_path    = '/content/drive/MyDrive/ML_Project/glove.6B.100d.txt'    # ← your path here\n",
        "embedding_dim = 100\n",
        "glove_index   = load_glove(glove_path, dim=embedding_dim)\n",
        "\n",
        "embedding_matrix = np.zeros((len(word_index)+1, embedding_dim))\n",
        "for w,i in word_index.items():\n",
        "    vec = glove_index.get(w)\n",
        "    if vec is not None:\n",
        "        embedding_matrix[i] = vec"
      ],
      "metadata": {
        "id": "2idOxaezb5cZ"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_binary_model(vocab_size, emb_dim, seq_len, emb_matrix):\n",
        "    inp = Input(shape=(seq_len,), name='input_ids')\n",
        "\n",
        "    # Embedding\n",
        "    x = Embedding(\n",
        "        vocab_size, emb_dim,\n",
        "        weights=[emb_matrix],\n",
        "        trainable=False\n",
        "    )(inp)\n",
        "\n",
        "    # Bi‑LSTM\n",
        "    x = Bidirectional(LSTM(128, return_sequences=True))(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "\n",
        "    # Attention\n",
        "    score   = Dense(1, activation='tanh')(x)     # (batch, seq, 1)\n",
        "    weights = Softmax(axis=1)(score)             # (batch, seq, 1)\n",
        "    context = Lambda(lambda z: K.sum(z[0]*z[1], axis=1))([weights, x])\n",
        "\n",
        "    # Classifier head\n",
        "    h = Dense(64, activation='relu')(context)\n",
        "    h = Dropout(0.3)(h)\n",
        "    out = Dense(1, activation='sigmoid')(h)\n",
        "\n",
        "    m = Model(inp, out)\n",
        "    m.compile(\n",
        "        loss='binary_crossentropy',\n",
        "        optimizer=tf.keras.optimizers.Adam(1e-4),\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return m"
      ],
      "metadata": {
        "id": "bOoDjXtNb9ex"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models, histories = {}, {}\n",
        "for task in ['IE','SN','TF','JP']:\n",
        "    print(f\"\\n=== Training {task} classifier ===\")\n",
        "    cw = compute_class_weight(\n",
        "        'balanced',\n",
        "        classes=np.unique(y_train[task]),\n",
        "        y=y_train[task]\n",
        "    )\n",
        "    class_weight = {i: w for i,w in enumerate(cw)}\n",
        "\n",
        "    m = build_binary_model(\n",
        "        vocab_size=len(word_index)+1,\n",
        "        emb_dim=embedding_dim,\n",
        "        seq_len=max_len,\n",
        "        emb_matrix=embedding_matrix\n",
        "    )\n",
        "\n",
        "    h = m.fit(\n",
        "        X_train, y_train[task],\n",
        "        validation_data=(X_test, y_test[task]),\n",
        "        epochs=10,\n",
        "        batch_size=64,\n",
        "        class_weight=class_weight,\n",
        "        callbacks=[\n",
        "            EarlyStopping('val_loss', patience=2, restore_best_weights=True),\n",
        "            ReduceLROnPlateau('val_loss', factor=0.5, patience=1)\n",
        "        ],\n",
        "        verbose=2\n",
        "    )\n",
        "    models[task]     = m\n",
        "    histories[task]  = h\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6BkKkc90cHjI",
        "outputId": "c3f0428f-999d-4d81-f681-403389a90538"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Training IE classifier ===\n",
            "Epoch 1/10\n",
            "102/102 - 13s - 129ms/step - accuracy: 0.5063 - loss: 0.6943 - val_accuracy: 0.2444 - val_loss: 0.7069 - learning_rate: 1.0000e-04\n",
            "Epoch 2/10\n",
            "102/102 - 20s - 192ms/step - accuracy: 0.5709 - loss: 0.6893 - val_accuracy: 0.5629 - val_loss: 0.6903 - learning_rate: 1.0000e-04\n",
            "Epoch 3/10\n",
            "102/102 - 10s - 102ms/step - accuracy: 0.5719 - loss: 0.6834 - val_accuracy: 0.3900 - val_loss: 0.7514 - learning_rate: 1.0000e-04\n",
            "Epoch 4/10\n",
            "102/102 - 10s - 94ms/step - accuracy: 0.5890 - loss: 0.6698 - val_accuracy: 0.6865 - val_loss: 0.6310 - learning_rate: 5.0000e-05\n",
            "Epoch 5/10\n",
            "102/102 - 10s - 98ms/step - accuracy: 0.6165 - loss: 0.6605 - val_accuracy: 0.6302 - val_loss: 0.6662 - learning_rate: 5.0000e-05\n",
            "Epoch 6/10\n",
            "102/102 - 10s - 100ms/step - accuracy: 0.6182 - loss: 0.6527 - val_accuracy: 0.6210 - val_loss: 0.6629 - learning_rate: 2.5000e-05\n",
            "\n",
            "=== Training SN classifier ===\n",
            "Epoch 1/10\n",
            "102/102 - 13s - 124ms/step - accuracy: 0.6204 - loss: 0.6929 - val_accuracy: 0.8423 - val_loss: 0.6694 - learning_rate: 1.0000e-04\n",
            "Epoch 2/10\n",
            "102/102 - 10s - 94ms/step - accuracy: 0.5905 - loss: 0.6903 - val_accuracy: 0.4832 - val_loss: 0.6934 - learning_rate: 1.0000e-04\n",
            "Epoch 3/10\n",
            "102/102 - 9s - 93ms/step - accuracy: 0.6165 - loss: 0.6835 - val_accuracy: 0.5196 - val_loss: 0.6887 - learning_rate: 5.0000e-05\n",
            "\n",
            "=== Training TF classifier ===\n",
            "Epoch 1/10\n",
            "102/102 - 14s - 133ms/step - accuracy: 0.5662 - loss: 0.6854 - val_accuracy: 0.6538 - val_loss: 0.6690 - learning_rate: 1.0000e-04\n",
            "Epoch 2/10\n",
            "102/102 - 9s - 93ms/step - accuracy: 0.6543 - loss: 0.6292 - val_accuracy: 0.7026 - val_loss: 0.5805 - learning_rate: 1.0000e-04\n",
            "Epoch 3/10\n",
            "102/102 - 10s - 101ms/step - accuracy: 0.7046 - loss: 0.5789 - val_accuracy: 0.7271 - val_loss: 0.5532 - learning_rate: 1.0000e-04\n",
            "Epoch 4/10\n",
            "102/102 - 10s - 99ms/step - accuracy: 0.7173 - loss: 0.5612 - val_accuracy: 0.7335 - val_loss: 0.5325 - learning_rate: 1.0000e-04\n",
            "Epoch 5/10\n",
            "102/102 - 10s - 100ms/step - accuracy: 0.7090 - loss: 0.5736 - val_accuracy: 0.6072 - val_loss: 0.6425 - learning_rate: 1.0000e-04\n",
            "Epoch 6/10\n",
            "102/102 - 10s - 100ms/step - accuracy: 0.7239 - loss: 0.5588 - val_accuracy: 0.7321 - val_loss: 0.5372 - learning_rate: 5.0000e-05\n",
            "\n",
            "=== Training JP classifier ===\n",
            "Epoch 1/10\n",
            "102/102 - 13s - 131ms/step - accuracy: 0.5197 - loss: 0.6920 - val_accuracy: 0.4357 - val_loss: 0.6968 - learning_rate: 1.0000e-04\n",
            "Epoch 2/10\n",
            "102/102 - 10s - 93ms/step - accuracy: 0.5180 - loss: 0.6903 - val_accuracy: 0.5800 - val_loss: 0.6860 - learning_rate: 1.0000e-04\n",
            "Epoch 3/10\n",
            "102/102 - 10s - 94ms/step - accuracy: 0.5656 - loss: 0.6843 - val_accuracy: 0.6044 - val_loss: 0.6689 - learning_rate: 1.0000e-04\n",
            "Epoch 4/10\n",
            "102/102 - 10s - 100ms/step - accuracy: 0.5865 - loss: 0.6779 - val_accuracy: 0.4864 - val_loss: 0.7052 - learning_rate: 1.0000e-04\n",
            "Epoch 5/10\n",
            "102/102 - 9s - 93ms/step - accuracy: 0.5812 - loss: 0.6710 - val_accuracy: 0.5915 - val_loss: 0.6670 - learning_rate: 5.0000e-05\n",
            "Epoch 6/10\n",
            "102/102 - 10s - 101ms/step - accuracy: 0.6051 - loss: 0.6666 - val_accuracy: 0.5168 - val_loss: 0.7009 - learning_rate: 5.0000e-05\n",
            "Epoch 7/10\n",
            "102/102 - 10s - 99ms/step - accuracy: 0.6013 - loss: 0.6637 - val_accuracy: 0.5505 - val_loss: 0.6856 - learning_rate: 2.5000e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n=== Test Accuracies ===\")\n",
        "for task, m in models.items():\n",
        "    loss, acc = m.evaluate(X_test, y_test[task], verbose=0)\n",
        "    print(f\"{task}: {acc:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "roDnI-I9prJT",
        "outputId": "8d4ad62e-6e34-41b9-8552-9449d4492cf7"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Test Accuracies ===\n",
            "IE: 0.686\n",
            "SN: 0.842\n",
            "TF: 0.734\n",
            "JP: 0.592\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred_bits = {}\n",
        "for task, m in models.items():\n",
        "    # shape (n_samples, 1) → flatten → 0 or 1 by 0.5 threshold\n",
        "    p = m.predict(X_test, batch_size=64)\n",
        "    pred_bits[task] = (p.flatten() >= 0.5).astype(int)\n",
        "\n",
        "bit2letter = {\n",
        "    'IE': {0:'I', 1:'E'},\n",
        "    'SN': {0:'S', 1:'N'},\n",
        "    'TF': {0:'T', 1:'F'},\n",
        "    'JP': {0:'J', 1:'P'},\n",
        "}\n",
        "\n",
        "pred_types = []\n",
        "for i in range(len(X_test)):\n",
        "    chars = [ bit2letter[task][pred_bits[task][i]]\n",
        "              for task in ['IE','SN','TF','JP'] ]\n",
        "    pred_types.append(''.join(chars))\n",
        "\n",
        "true_types = test_df['type'].values\n",
        "overall_acc = np.mean(np.array(pred_types) == true_types)\n",
        "print(f\"Overall 4‑letter accuracy: {overall_acc:.3f}\")\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "print(classification_report(true_types, pred_types))\n",
        "cm = confusion_matrix(true_types, pred_types, labels=np.unique(true_types))\n",
        "print(\"16‑way confusion matrix:\\n\", cm)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "THAPFz4asKsK",
        "outputId": "74b4389b-5c81-4ba7-ac00-4baf78700905"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 39ms/step\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 42ms/step\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 42ms/step\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 39ms/step\n",
            "Overall 4‑letter accuracy: 0.260\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        ENFJ       0.10      0.10      0.10        48\n",
            "        ENFP       0.18      0.34      0.24       169\n",
            "        ENTJ       0.05      0.09      0.06        58\n",
            "        ENTP       0.23      0.14      0.18       171\n",
            "        ESFJ       0.00      0.00      0.00        10\n",
            "        ESFP       0.05      0.17      0.08        12\n",
            "        ESTJ       0.00      0.00      0.00        10\n",
            "        ESTP       0.00      0.00      0.00        22\n",
            "        INFJ       0.29      0.27      0.28       368\n",
            "        INFP       0.35      0.41      0.37       458\n",
            "        INTJ       0.26      0.42      0.32       273\n",
            "        INTP       0.36      0.21      0.27       326\n",
            "        ISFJ       0.00      0.00      0.00        41\n",
            "        ISFP       0.05      0.01      0.02        68\n",
            "        ISTJ       0.00      0.00      0.00        51\n",
            "        ISTP       0.17      0.02      0.04        84\n",
            "\n",
            "    accuracy                           0.26      2169\n",
            "   macro avg       0.13      0.14      0.12      2169\n",
            "weighted avg       0.25      0.26      0.25      2169\n",
            "\n",
            "16‑way confusion matrix:\n",
            " [[  5  12   0   2   0   2   0   0  11  12   3   0   0   1   0   0]\n",
            " [  6  57   8   7   0   7   0   0  22  46  10   5   0   1   0   0]\n",
            " [  3  12   5   3   0   0   0   0   8   4  18   5   0   0   0   0]\n",
            " [  3  26  10  24   0   2   0   3  14  20  48  19   0   1   0   1]\n",
            " [  1   2   0   0   0   0   0   0   5   1   1   0   0   0   0   0]\n",
            " [  1   1   0   2   0   2   0   0   2   1   0   2   0   0   1   0]\n",
            " [  0   0   0   1   0   0   0   1   3   1   3   1   0   0   0   0]\n",
            " [  0   9   3   4   0   0   0   0   0   2   4   0   0   0   0   0]\n",
            " [ 11  42  17   6   0   7   0   2  98 126  43  13   0   2   0   1]\n",
            " [ 12  74   9   8   0   8   1   3  91 187  31  22   0  10   0   2]\n",
            " [  4  26  23  13   0   1   0   1  33  19 114  34   1   0   0   4]\n",
            " [  1  18  14  15   0   3   0   0  29  56 117  69   0   2   0   2]\n",
            " [  1   9   2   2   1   0   0   0   8  13   5   0   0   0   0   0]\n",
            " [  0  14   2   4   0   4   0   1   4  31   5   2   0   1   0   0]\n",
            " [  2   5   3   3   0   0   0   1   5  12  13   5   0   2   0   0]\n",
            " [  2   8   6   9   0   1   0   5   5  11  16  17   0   2   0   2]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f1_macro    = f1_score(true_types, pred_types, average='macro')\n",
        "f1_weighted = f1_score(true_types, pred_types, average='weighted')\n",
        "\n",
        "print(f\"16‑class Macro F1:    {f1_macro:.4f}\")\n",
        "print(f\"16‑class Weighted F1: {f1_weighted:.4f}\")\n",
        "\n",
        "le16      = LabelEncoder().fit(data['type'])\n",
        "classes16 = le16.classes_\n",
        "y_true16  = le16.transform(true_types)\n",
        "\n",
        "p_IE = models['IE'].predict(X_test).flatten()\n",
        "p_SN = models['SN'].predict(X_test).flatten()\n",
        "p_TF = models['TF'].predict(X_test).flatten()\n",
        "p_JP = models['JP'].predict(X_test).flatten()\n",
        "\n",
        "n_samples = X_test.shape[0]\n",
        "probs16   = np.zeros((n_samples, len(classes16)))\n",
        "\n",
        "for j, mbti in enumerate(classes16):\n",
        "    bitE = 1 if mbti[0]=='E' else 0\n",
        "    bitN = 1 if mbti[1]=='N' else 0\n",
        "    bitF = 1 if mbti[2]=='F' else 0\n",
        "    bitP = 1 if mbti[3]=='P' else 0\n",
        "\n",
        "    p0 =    p_IE if bitE else (1-p_IE)\n",
        "    p1 =    p_SN if bitN else (1-p_SN)\n",
        "    p2 =    p_TF if bitF else (1-p_TF)\n",
        "    p3 =    p_JP if bitP else (1-p_JP)\n",
        "\n",
        "    probs16[:, j] = p0 * p1 * p2 * p3\n",
        "\n",
        "y_true_onehot = tf.keras.utils.to_categorical(y_true16, num_classes=len(classes16))\n",
        "\n",
        "auc = roc_auc_score(y_true_onehot, probs16,\n",
        "                            average='macro',   multi_class='ovr')\n",
        "\n",
        "print(f\"16‑class AUC:    {auc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dxLlwM7bshyN",
        "outputId": "24d7a1b9-2a6f-4e3c-f4b2-02f27b23d313"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16‑class Macro F1:    0.1225\n",
            "16‑class Weighted F1: 0.2455\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step\n",
            "16‑class AUC:    0.6875\n"
          ]
        }
      ]
    }
  ]
}